{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed826ed1-5152-4b4c-b961-b744f793d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "ds=pd.read_csv(r\"C:\\Users\\Zunnurain.Badar\\IMDB dataset.csv\")\n",
    "print(ds.isnull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf47f9b-4aae-4006-839e-1fa19f56c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing columns of ds\n",
    "ds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f55f0-a60b-430d-883c-0b71d4c07e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing lowercasing\n",
    "ds['review'] = ds['review'].str.lower()\n",
    "ds['sentiment'] = ds['sentiment'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1069d7-4103-4330-b9cb-bc8b53ba46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2bcc6-ea5c-4788-a26b-b451e0b859fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "tokens = ds['review'].astype(str).map(word_tokenize)\n",
    "print (tokens.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7990d2-e24f-4ff9-a856-15a0ac18878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae42eda-396b-4471-9429-9c41c850938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the stopwords\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(filtered_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37282d00-1fa3-4731-857c-b74caddcebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#veectorization through tfidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_data = ds['review'].tolist()\n",
    "tfidf = TfidfVectorizer(max_features = 20000)\n",
    "tfidf_matrix = tfidf.fit_transform(text_data)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7f54f-837e-4fe1-b397-ff9190108d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = tfidf_matrix\n",
    "y = ds['sentiment']\n",
    "x_train , x_test , y_train, y_test = train_test_split(x , y , test_size = 0.2, random_state = 42)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922aadd4-3a6a-4df3-ba51-861e21d795b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking accuracy , f1 score, recall\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "accuracy = accuracy_score (y_pred,y_test)\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(\"classification_report:\" , classification_report(y_pred,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be146d3-f2f9-4090-a415-9557173467c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model and vectorizer for deployment\n",
    "import joblib\n",
    "joblib.dump(model , \"model.pkl\")\n",
    "joblib.dump(tfidf , \"vectorizer.pkl\")\n",
    "print(\"model and vectorizer saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711338e-e453-4f80-87b3-9bb74b332a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2764326-8af5-41b9-8760-2ffe694c6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import joblib\n",
    "model = joblib.load(\"model.pkl\")\n",
    "vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "st.title(\"Sentiment Analysis App üòäüò†\")\n",
    "user_input = st.text_area(\"Enter your text:\")\n",
    "if st.button(\"Analyze Sentiment\"):\n",
    "    if user_input:\n",
    "        transformed_text = vectorizer.transform([user_input])\n",
    "        prediction = model.predict(transformed_text)\n",
    "        sentiment = \"Positive üòÉ\" if prediction[0] == \"positive\" else \"Negative üò†\"\n",
    "        st.write(\"Sentiment:\", sentiment)\n",
    "    else:\n",
    "        st.warning(\"‚ö†Ô∏è Please enter some text!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bacbf5-98ac-4b57-88c8-2909fda972bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d9f97-6e92-462d-aedb-58704fce2bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
